project:
  seed: 42
  output_dir: "outputs/distilbert_tab"
  logging_dir: "outputs/logs"
  run_name: "distilbert-tab-ner"

model:
  pretrained_name: "distilbert-base-uncased"
  max_length: 384
  stride: 128

data:
  local_data_dir: "data"
  train_file: "train.json"
  val_file: "validation.json"
  test_file: "test.json"

  # fields inside each JSON record
  text_field: "text"
  mentions_field: "entity_mentions"

labels:
  scheme: "bio"
  use_type_specific: true
  entity_types:
    - "PERSON"
    - "CODE"
    - "DATETIME"
    - "LOC"
    - "ORG"

training:
  num_train_epochs: 3
  learning_rate: 5e-5
  weight_decay: 0.01
  warmup_ratio: 0.06

  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 2

  evaluation_strategy: "steps"
  eval_steps: 200

  save_strategy: "steps"
  save_steps: 200
  save_total_limit: 2

  logging_strategy: "steps"
  logging_steps: 50

  load_best_model_at_end: true
  metric_for_best_model: "f1"
  greater_is_better: true

  fp16: true
  bf16: false

  dataloader_num_workers: 2
  report_to: []

evaluation:
  metric: "seqeval"
  ignore_labels: ["O"]

inference:
  placeholder_style: "type"
  score_threshold: 0.0
